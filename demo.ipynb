{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7be0a93-dba8-4efb-a79a-bf4b691f1747",
   "metadata": {},
   "source": [
    "### QA Bot: A PDF QA Assistant\n",
    "\n",
    "The main objective of this notebook is to demonstrate how to use the LLM to create the QA bot for PDF files. The process can break into a number of steps:\n",
    "1. Data Preprocessing - PDF files are processed to extract their textual content\n",
    "2. Content Chunking - The text content is divided into fixed-size chunks of 512 tokens, overlapping 100 tokens. \n",
    "3. Embeddings Generation - Each content chunk is transformed into embeddings using ```e5-base-b2``` model. (Any other suitable model can be used here)\n",
    "4. Building the QA bot: ```google/flan-t5-large``` LLM model was selected for this task.\n",
    "\n",
    "\n",
    "Note that in this demo, I did not enhance the Conversational Memory. For those reader who are interested in this topic, please take a look about [4]\n",
    "\n",
    "<img src=\"overview.png\"  width=\"600\" height=\"300\">\n",
    "\n",
    "### PDF file sources\n",
    "- https://en.wikipedia.org/wiki/Large_language_model\n",
    "- https://en.wikipedia.org/wiki/ChatGPT\n",
    "- https://en.wikipedia.org/wiki/Stable_Diffusion\n",
    "- https://en.wikipedia.org/wiki/Midjourney\n",
    "\n",
    "\n",
    "\n",
    "#### References\n",
    "[1] - Chunking Strategries for LLM Applications:\n",
    "   https://www.pinecone.io/learn/chunking-strategies/\n",
    "   \n",
    "[2] - Massive Text Embedding Benchmark (MTEB) Leaderboard - https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "[3] - Converstaional Retrieval QA - https://python.langchain.com/docs/modules/chains/popular/chat_vector_db\n",
    "\n",
    "[4] - Conversational Memory for LLMs with Langchain - https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4dd488b-b77b-4b58-8050-0f115ed388f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3b1b8-3fb3-46e3-aa69-d66083f99832",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70ac868-f16f-4485-9205-94e0ac28a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = []\n",
    "metadatas = []\n",
    "fs = glob.glob(\"./files/*.pdf\")\n",
    "\n",
    "for f in fs:\n",
    "    reader = PdfReader(f)\n",
    "    for p in reader.pages:\n",
    "        file_content.append(p.extract_text())\n",
    "        metadatas.append({\"source\": os.path.basename(f)})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c1622-1a30-401a-8238-7aa957757c09",
   "metadata": {},
   "source": [
    "### 2. Content Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d368cb-52f6-47d7-bef6-35e51e4a9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 100\n",
    ")\n",
    "\n",
    "lst_chunk = text_splitter.create_documents(file_content, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402be88e-29c2-4c7a-80d1-c023c2955dbc",
   "metadata": {},
   "source": [
    "### 3. Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41156cf9-ecdb-44f7-b628-601bf6786278",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/e5-base-v2\") \n",
    "\n",
    "db = FAISS.from_documents(lst_chunk, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8dcbf1-46c4-44f0-9399-c2a3abe8277f",
   "metadata": {},
   "source": [
    "### 4. QA Bot Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25979f44-96b6-4233-856f-9a148384a6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = \"google/flan-t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3, \"score_threshold\": 0.9}) # top 3, threshold: 0.9\n",
    "\n",
    "pl = pipeline(\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            model_kwargs={\"max_length\": 512, \"temperature\": 0.0},\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "hf_llm = HuggingFacePipeline(pipeline=pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4042eb6c-0b09-4b1a-a810-062f2d51e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_system_template = r\"\"\" \n",
    "Given a specific context, please give a short answer to the question. \n",
    "If you cannot find a proper ansser from the context, just say that I don't know, don't try to make up an answer. \n",
    " ----\n",
    "{context}\n",
    "----\n",
    "\"\"\"\n",
    "general_user_template = \"Question:```{question}```\"\n",
    "messages = [\n",
    "            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "]\n",
    "qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=hf_llm,\n",
    "        retriever=retriever, \n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc8633-8d50-4a6c-8230-fe68f803bc3c",
   "metadata": {},
   "source": [
    "### Chat with QA Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3085fbfd-b539-4556-bb70-b7bb82aec812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the ChatGPT?\",\n",
       " 'chat_history': [HumanMessage(content=\"What's the ChatGPT?\", additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Chat Generative Pre-Trained Transformer', additional_kwargs={}, example=False)],\n",
       " 'answer': 'Chat Generative Pre-Trained Transformer'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain({'question': \"What's the ChatGPT?\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce368d95-857a-48b8-b9fc-acac750616f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the latest engine version of ChatGPT?\",\n",
       " 'chat_history': [HumanMessage(content=\"What's the ChatGPT?\", additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Chat Generative Pre-Trained Transformer', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content=\"What's the latest engine version of ChatGPT?\", additional_kwargs={}, example=False),\n",
       "  AIMessage(content='GPT-4', additional_kwargs={}, example=False)],\n",
       " 'answer': 'GPT-4'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain({'question': \"What's the latest engine version of ChatGPT?\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3feb6fae-b586-4dc2-aff8-d686fc1c5b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the major difference between ChatGPT 3.5 and 4.0',\n",
       " 'chat_history': [HumanMessage(content=\"What's the ChatGPT?\", additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Chat Generative Pre-Trained Transformer', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content=\"What's the latest engine version of ChatGPT?\", additional_kwargs={}, example=False),\n",
       "  AIMessage(content='GPT-4', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is the major difference between ChatGPT 3.5 and 4.0', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"I don't know\", additional_kwargs={}, example=False)],\n",
       " 'answer': \"I don't know\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain({'question': \"what is the major difference between ChatGPT 3.5 and 4.0\"}) # show I don't know if not enough info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
